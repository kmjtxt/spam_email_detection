{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"../data/processed/training.csv\")\n",
    "testing = pd.read_csv(\"../data/processed/testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: off site with john griebling ' s opti...</td>\n",
       "      <td>subject off site with john griebling s optical...</td>\n",
       "      <td>0</td>\n",
       "      <td>['subject', 'site', 'john', 'griebling', 'opti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Author: metze\\nDate: 2007-04-27 14:23:08 +0000...</td>\n",
       "      <td>author metze new revision websvn log pass down...</td>\n",
       "      <td>0</td>\n",
       "      <td>['author', 'metze', 'new', 'revision', 'websvn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  Subject: off site with john griebling ' s opti...   \n",
       "1  Author: metze\\nDate: 2007-04-27 14:23:08 +0000...   \n",
       "\n",
       "                                        cleaned_body  label  \\\n",
       "0  subject off site with john griebling s optical...      0   \n",
       "1  author metze new revision websvn log pass down...      0   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['subject', 'site', 'john', 'griebling', 'opti...  \n",
       "1  ['author', 'metze', 'new', 'revision', 'websvn...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john ,\\nthe more i look at these comparisons ,...</td>\n",
       "      <td>john the more i look at these comparisons the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['john', 'look', 'comparison', 'convinced', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On Friday 22 February 2008 15:46:06 Philipp Th...</td>\n",
       "      <td>on friday february philipp thomas wrote stan g...</td>\n",
       "      <td>0</td>\n",
       "      <td>['friday', 'february', 'philipp', 'thomas', 'w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  john ,\\nthe more i look at these comparisons ,...   \n",
       "1  On Friday 22 February 2008 15:46:06 Philipp Th...   \n",
       "\n",
       "                                        cleaned_body  label  \\\n",
       "0  john the more i look at these comparisons the ...      0   \n",
       "1  on friday february philipp thomas wrote stan g...      0   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['john', 'look', 'comparison', 'convinced', 'a...  \n",
       "1  ['friday', 'february', 'philipp', 'thomas', 'w...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from scipy.sparse import hstack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = training['cleaned_body']\n",
    "y = training['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=1000, stop_words='english', min_df = 5, max_df = 0.8)\n",
    "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 5), max_features=500, min_df = 10, max_df = 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_grams = word_vectorizer.fit_transform(X_text)\n",
    "X_char_grams = char_vectorizer.fit_transform(X_text)\n",
    "\n",
    "X_combined_sparse = hstack([X_word_grams, X_char_grams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word features: 1000\n",
      "First 5 word features: ['able', 'access', 'according', 'account', 'act']\n",
      "\n",
      "Number of character features: 500\n",
      "First 10 character features: [' ac', ' ad', ' al', ' all', ' ar', ' are', ' are ', ' as', ' as ', ' at']\n",
      "\n",
      "Total number of combined features: 1500\n",
      "Corresponds to X_combined_sparse shape: (100784, 1500)\n"
     ]
    }
   ],
   "source": [
    "word_feature_names = word_vectorizer.get_feature_names_out()\n",
    "char_feature_names = char_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of word features: {len(word_feature_names)}\")\n",
    "print(f\"First 5 word features: {list(word_feature_names[:5])}\")\n",
    "\n",
    "print(f\"\\nNumber of character features: {len(char_feature_names)}\")\n",
    "print(f\"First 10 character features: {list(char_feature_names[:10])}\")\n",
    "\n",
    "combined_feature_names = list(word_feature_names) + list(char_feature_names)\n",
    "\n",
    "print(f\"\\nTotal number of combined features: {len(combined_feature_names)}\")\n",
    "print(f\"Corresponds to X_combined_sparse shape: {X_combined_sparse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Words (CountVectorizer) ---\n",
      "         feature  count\n",
      "242        enron  49896\n",
      "235        email  47786\n",
      "542          new  45398\n",
      "444         list  37245\n",
      "227          ect  36452\n",
      "883         time  34488\n",
      "435         like  33539\n",
      "919          use  32918\n",
      "403         just  30101\n",
      "371  information  29860\n",
      "138      company  29138\n",
      "501      message  28680\n",
      "831      subject  28321\n",
      "411         know  26446\n",
      "537         need  25529\n",
      "473         make  24550\n",
      "990        wrote  24116\n",
      "736         said  24055\n",
      "77      business  23582\n",
      "119          com  22664\n"
     ]
    }
   ],
   "source": [
    "word_feature_names = word_vectorizer.get_feature_names_out()\n",
    "word_counts = np.array(X_word_grams.sum(axis=0)).flatten()\n",
    "word_feature_counts = pd.DataFrame({'feature': word_feature_names, 'count': word_counts})\n",
    "print(\"--- Top 20 Words (CountVectorizer) ---\")\n",
    "print(word_feature_counts.sort_values(by='count', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Characters (CountVectorizer) ---\n",
      "    feature   count\n",
      "331     of   481911\n",
      "64      of   476954\n",
      "461     tio  438978\n",
      "462    tion  435982\n",
      "271    ion   375604\n",
      "404     s t  357179\n",
      "98       wi  345624\n",
      "46      in   339519\n",
      "434     t t  312277\n",
      "116     al   312196\n",
      "197     e s  310281\n",
      "135     ati  309539\n",
      "186     e i  304541\n",
      "308     n t  299890\n",
      "168     d t  299854\n",
      "352     our  298447\n",
      "130     as   295858\n",
      "77       se  294534\n",
      "442     ter  292531\n",
      "66       on  287379\n"
     ]
    }
   ],
   "source": [
    "char_feature_names = char_vectorizer.get_feature_names_out()\n",
    "char_counts = np.array(X_char_grams.sum(axis=0)).flatten()\n",
    "char_feature_counts = pd.DataFrame({'feature': char_feature_names, 'count': char_counts})\n",
    "print(\"--- Top 20 Characters (CountVectorizer) ---\")\n",
    "print(char_feature_counts.sort_values(by='count', ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer_tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=1000, stop_words='english', min_df = 5, max_df = 0.8, sublinear_tf=True)\n",
    "char_vectorizer_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), max_features=500, min_df = 10, max_df = 0.7, sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_grams_tfidf = word_vectorizer_tfidf.fit_transform(X_text)\n",
    "X_char_grams_tfidf = char_vectorizer_tfidf.fit_transform(X_text)\n",
    "\n",
    "X_combined_sparse_tfidf = hstack([X_word_grams_tfidf, X_char_grams_tfidf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word features: 1000\n",
      "First 5 word features: ['able', 'access', 'according', 'account', 'act']\n",
      "\n",
      "Number of character features: 500\n",
      "First 10 character features: [' ac', ' ad', ' al', ' all', ' ar', ' are', ' are ', ' as', ' as ', ' at']\n",
      "\n",
      "Total number of combined features: 1500\n",
      "Corresponds to X_combined_sparse shape: (100784, 1500)\n"
     ]
    }
   ],
   "source": [
    "word_feature_names_tfidf = word_vectorizer_tfidf.get_feature_names_out()\n",
    "char_feature_names_tfidf = char_vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "print(f\"Number of word features: {len(word_feature_names_tfidf)}\")\n",
    "print(f\"First 5 word features: {list(word_feature_names_tfidf[:5])}\")\n",
    "\n",
    "print(f\"\\nNumber of character features: {len(char_feature_names_tfidf)}\")\n",
    "print(f\"First 10 character features: {list(char_feature_names_tfidf[:10])}\")\n",
    "\n",
    "combined_feature_names_tfidf = list(word_feature_names_tfidf) + list(char_feature_names_tfidf)\n",
    "\n",
    "print(f\"\\nTotal number of combined features: {len(combined_feature_names_tfidf)}\")\n",
    "print(f\"Corresponds to X_combined_sparse shape: {X_combined_sparse_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Words (TF-IDF) ---\n",
      "       word_ngram  total_tfidf_score\n",
      "235         email        2659.547459\n",
      "444          list        2273.415760\n",
      "542           new        2264.969548\n",
      "435          like        2232.482608\n",
      "403          just        2097.829382\n",
      "883          time        2092.696300\n",
      "990         wrote        2020.873956\n",
      "919           use        1982.633000\n",
      "411          know        1862.635129\n",
      "872        thanks        1813.379287\n",
      "537          need        1811.614455\n",
      "501       message        1783.396731\n",
      "831       subject        1777.712725\n",
      "468       mailing        1673.539987\n",
      "946          want        1669.773026\n",
      "473          make        1641.420793\n",
      "469  mailing list        1614.785668\n",
      "178           day        1602.618789\n",
      "58           best        1597.909289\n",
      "371   information        1588.924414\n"
     ]
    }
   ],
   "source": [
    "word_total_tfidf = np.array(X_word_grams_tfidf.sum(axis=0)).flatten()\n",
    "word_tfidf_df = pd.DataFrame({\n",
    "    'word_ngram': word_feature_names_tfidf,\n",
    "    'total_tfidf_score': word_total_tfidf\n",
    "}).sort_values(by='total_tfidf_score', ascending=False)\n",
    "\n",
    "print(f\"--- Top 20 Words (TF-IDF) ---\")\n",
    "print(word_tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Characters (TF-IDF) ---\n",
      "    char_ngram  total_tfidf_score\n",
      "352        our        4444.030340\n",
      "331        of         4415.704431\n",
      "461        tio        4406.678704\n",
      "462       tion        4394.294327\n",
      "64         of         4389.544432\n",
      "347        ou         4325.345953\n",
      "497       you         4214.261110\n",
      "107       you         4184.587768\n",
      "98          wi        4154.863913\n",
      "197        e s        4089.655639\n",
      "66          on        4078.672149\n",
      "404        s t        4055.727421\n",
      "186        e i        4049.690988\n",
      "271       ion         4033.976709\n",
      "474        ur         4028.414592\n",
      "410        se         4028.243098\n",
      "434        t t        4023.361578\n",
      "353       our         3991.401200\n",
      "227       for         3985.738914\n",
      "31        for         3971.779149\n"
     ]
    }
   ],
   "source": [
    "char_total_tfidf = np.array(X_char_grams_tfidf.sum(axis=0)).flatten()\n",
    "char_tfidf_df = pd.DataFrame({\n",
    "    'char_ngram': char_feature_names_tfidf,\n",
    "    'total_tfidf_score': char_total_tfidf\n",
    "}).sort_values(by='total_tfidf_score', ascending=False)\n",
    "\n",
    "print(f\"--- Top 20 Characters (TF-IDF) ---\")\n",
    "print(char_tfidf_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "- TF-IDF is able to ignore noise in data and provide more insightful for analysis by words\n",
    "- Similar results for characters analysis\n",
    "\n",
    "Hence using TF-IDF should be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>body_tfidf</th>\n",
       "      <th>body_char_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: off site with john griebling ' s opti...</td>\n",
       "      <td>subject off site with john griebling s optical...</td>\n",
       "      <td>0</td>\n",
       "      <td>['subject', 'site', 'john', 'griebling', 'opti...</td>\n",
       "      <td>(0, 831)\\t0.07180768530448348\\n  (0, 790)\\t0...</td>\n",
       "      <td>(0, 424)\\t0.03481832019111312\\n  (0, 206)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Author: metze\\nDate: 2007-04-27 14:23:08 +0000...</td>\n",
       "      <td>author metze new revision websvn log pass down...</td>\n",
       "      <td>0</td>\n",
       "      <td>['author', 'metze', 'new', 'revision', 'websvn...</td>\n",
       "      <td>(0, 671)\\t0.43648017825283475\\n  (0, 48)\\t0....</td>\n",
       "      <td>(0, 439)\\t0.13160465654773695\\n  (0, 146)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with the holidays quickly approaching , enron ...</td>\n",
       "      <td>with the holidays quickly approaching enron tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>['holiday', 'quickly', 'approaching', 'enron',...</td>\n",
       "      <td>(0, 365)\\t0.10585982325492739\\n  (0, 601)\\t0...</td>\n",
       "      <td>(0, 159)\\t0.05733874751940442\\n  (0, 431)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Bayesian AI group at Monash University is ...</td>\n",
       "      <td>the bayesian ai group at monash university is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['bayesian', 'ai', 'group', 'monash', 'univers...</td>\n",
       "      <td>(0, 541)\\t0.25803630820234624\\n  (0, 36)\\t0....</td>\n",
       "      <td>(0, 416)\\t0.07890075252138798\\n  (0, 290)\\t0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Meyer wrote:\\n&gt; Trying to install it from...</td>\n",
       "      <td>mike meyer wrote trying to install it from the...</td>\n",
       "      <td>0</td>\n",
       "      <td>['mike', 'meyer', 'wrote', 'trying', 'install'...</td>\n",
       "      <td>(0, 542)\\t0.13741301079189092\\n  (0, 912)\\t0...</td>\n",
       "      <td>(0, 424)\\t0.07236663126405836\\n  (0, 431)\\t0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  Subject: off site with john griebling ' s opti...   \n",
       "1  Author: metze\\nDate: 2007-04-27 14:23:08 +0000...   \n",
       "2  with the holidays quickly approaching , enron ...   \n",
       "3  The Bayesian AI group at Monash University is ...   \n",
       "4  Mike Meyer wrote:\\n> Trying to install it from...   \n",
       "\n",
       "                                        cleaned_body  label  \\\n",
       "0  subject off site with john griebling s optical...      0   \n",
       "1  author metze new revision websvn log pass down...      0   \n",
       "2  with the holidays quickly approaching enron tr...      0   \n",
       "3  the bayesian ai group at monash university is ...      0   \n",
       "4  mike meyer wrote trying to install it from the...      0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['subject', 'site', 'john', 'griebling', 'opti...   \n",
       "1  ['author', 'metze', 'new', 'revision', 'websvn...   \n",
       "2  ['holiday', 'quickly', 'approaching', 'enron',...   \n",
       "3  ['bayesian', 'ai', 'group', 'monash', 'univers...   \n",
       "4  ['mike', 'meyer', 'wrote', 'trying', 'install'...   \n",
       "\n",
       "                                          body_tfidf  \\\n",
       "0    (0, 831)\\t0.07180768530448348\\n  (0, 790)\\t0...   \n",
       "1    (0, 671)\\t0.43648017825283475\\n  (0, 48)\\t0....   \n",
       "2    (0, 365)\\t0.10585982325492739\\n  (0, 601)\\t0...   \n",
       "3    (0, 541)\\t0.25803630820234624\\n  (0, 36)\\t0....   \n",
       "4    (0, 542)\\t0.13741301079189092\\n  (0, 912)\\t0...   \n",
       "\n",
       "                                     body_char_tfidf  \n",
       "0    (0, 424)\\t0.03481832019111312\\n  (0, 206)\\t0...  \n",
       "1    (0, 439)\\t0.13160465654773695\\n  (0, 146)\\t0...  \n",
       "2    (0, 159)\\t0.05733874751940442\\n  (0, 431)\\t0...  \n",
       "3    (0, 416)\\t0.07890075252138798\\n  (0, 290)\\t0...  \n",
       "4    (0, 424)\\t0.07236663126405836\\n  (0, 431)\\t0...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_add = csr_matrix(X_word_grams_tfidf)\n",
    "training['body_tfidf'] = [row for row in to_add]\n",
    "\n",
    "to_add2 = csr_matrix(X_char_grams_tfidf)\n",
    "training['body_char_tfidf'] = [row for row in to_add2]\n",
    "\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>body_tfidf</th>\n",
       "      <th>body_char_tfidf</th>\n",
       "      <th>body_countvec</th>\n",
       "      <th>body_char_countvec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: off site with john griebling ' s opti...</td>\n",
       "      <td>subject off site with john griebling s optical...</td>\n",
       "      <td>0</td>\n",
       "      <td>['subject', 'site', 'john', 'griebling', 'opti...</td>\n",
       "      <td>(0, 831)\\t0.07180768530448348\\n  (0, 790)\\t0...</td>\n",
       "      <td>(0, 424)\\t0.03481832019111312\\n  (0, 206)\\t0...</td>\n",
       "      <td>(0, 831)\\t1\\n  (0, 790)\\t1\\n  (0, 397)\\t5\\n ...</td>\n",
       "      <td>(0, 424)\\t1\\n  (0, 206)\\t5\\n  (0, 159)\\t3\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Author: metze\\nDate: 2007-04-27 14:23:08 +0000...</td>\n",
       "      <td>author metze new revision websvn log pass down...</td>\n",
       "      <td>0</td>\n",
       "      <td>['author', 'metze', 'new', 'revision', 'websvn...</td>\n",
       "      <td>(0, 671)\\t0.43648017825283475\\n  (0, 48)\\t0....</td>\n",
       "      <td>(0, 439)\\t0.13160465654773695\\n  (0, 146)\\t0...</td>\n",
       "      <td>(0, 671)\\t4\\n  (0, 48)\\t1\\n  (0, 542)\\t1\\n  ...</td>\n",
       "      <td>(0, 439)\\t8\\n  (0, 146)\\t5\\n  (0, 60)\\t1\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with the holidays quickly approaching , enron ...</td>\n",
       "      <td>with the holidays quickly approaching enron tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>['holiday', 'quickly', 'approaching', 'enron',...</td>\n",
       "      <td>(0, 365)\\t0.10585982325492739\\n  (0, 601)\\t0...</td>\n",
       "      <td>(0, 159)\\t0.05733874751940442\\n  (0, 431)\\t0...</td>\n",
       "      <td>(0, 365)\\t1\\n  (0, 601)\\t1\\n  (0, 289)\\t1\\n ...</td>\n",
       "      <td>(0, 159)\\t2\\n  (0, 431)\\t1\\n  (0, 276)\\t2\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Bayesian AI group at Monash University is ...</td>\n",
       "      <td>the bayesian ai group at monash university is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['bayesian', 'ai', 'group', 'monash', 'univers...</td>\n",
       "      <td>(0, 541)\\t0.25803630820234624\\n  (0, 36)\\t0....</td>\n",
       "      <td>(0, 416)\\t0.07890075252138798\\n  (0, 290)\\t0...</td>\n",
       "      <td>(0, 541)\\t1\\n  (0, 36)\\t1\\n  (0, 320)\\t1\\n  ...</td>\n",
       "      <td>(0, 416)\\t1\\n  (0, 290)\\t1\\n  (0, 252)\\t2\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mike Meyer wrote:\\n&gt; Trying to install it from...</td>\n",
       "      <td>mike meyer wrote trying to install it from the...</td>\n",
       "      <td>0</td>\n",
       "      <td>['mike', 'meyer', 'wrote', 'trying', 'install'...</td>\n",
       "      <td>(0, 542)\\t0.13741301079189092\\n  (0, 912)\\t0...</td>\n",
       "      <td>(0, 424)\\t0.07236663126405836\\n  (0, 431)\\t0...</td>\n",
       "      <td>(0, 542)\\t1\\n  (0, 912)\\t1\\n  (0, 509)\\t1\\n ...</td>\n",
       "      <td>(0, 424)\\t1\\n  (0, 431)\\t2\\n  (0, 416)\\t2\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  Subject: off site with john griebling ' s opti...   \n",
       "1  Author: metze\\nDate: 2007-04-27 14:23:08 +0000...   \n",
       "2  with the holidays quickly approaching , enron ...   \n",
       "3  The Bayesian AI group at Monash University is ...   \n",
       "4  Mike Meyer wrote:\\n> Trying to install it from...   \n",
       "\n",
       "                                        cleaned_body  label  \\\n",
       "0  subject off site with john griebling s optical...      0   \n",
       "1  author metze new revision websvn log pass down...      0   \n",
       "2  with the holidays quickly approaching enron tr...      0   \n",
       "3  the bayesian ai group at monash university is ...      0   \n",
       "4  mike meyer wrote trying to install it from the...      0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['subject', 'site', 'john', 'griebling', 'opti...   \n",
       "1  ['author', 'metze', 'new', 'revision', 'websvn...   \n",
       "2  ['holiday', 'quickly', 'approaching', 'enron',...   \n",
       "3  ['bayesian', 'ai', 'group', 'monash', 'univers...   \n",
       "4  ['mike', 'meyer', 'wrote', 'trying', 'install'...   \n",
       "\n",
       "                                          body_tfidf  \\\n",
       "0    (0, 831)\\t0.07180768530448348\\n  (0, 790)\\t0...   \n",
       "1    (0, 671)\\t0.43648017825283475\\n  (0, 48)\\t0....   \n",
       "2    (0, 365)\\t0.10585982325492739\\n  (0, 601)\\t0...   \n",
       "3    (0, 541)\\t0.25803630820234624\\n  (0, 36)\\t0....   \n",
       "4    (0, 542)\\t0.13741301079189092\\n  (0, 912)\\t0...   \n",
       "\n",
       "                                     body_char_tfidf  \\\n",
       "0    (0, 424)\\t0.03481832019111312\\n  (0, 206)\\t0...   \n",
       "1    (0, 439)\\t0.13160465654773695\\n  (0, 146)\\t0...   \n",
       "2    (0, 159)\\t0.05733874751940442\\n  (0, 431)\\t0...   \n",
       "3    (0, 416)\\t0.07890075252138798\\n  (0, 290)\\t0...   \n",
       "4    (0, 424)\\t0.07236663126405836\\n  (0, 431)\\t0...   \n",
       "\n",
       "                                       body_countvec  \\\n",
       "0    (0, 831)\\t1\\n  (0, 790)\\t1\\n  (0, 397)\\t5\\n ...   \n",
       "1    (0, 671)\\t4\\n  (0, 48)\\t1\\n  (0, 542)\\t1\\n  ...   \n",
       "2    (0, 365)\\t1\\n  (0, 601)\\t1\\n  (0, 289)\\t1\\n ...   \n",
       "3    (0, 541)\\t1\\n  (0, 36)\\t1\\n  (0, 320)\\t1\\n  ...   \n",
       "4    (0, 542)\\t1\\n  (0, 912)\\t1\\n  (0, 509)\\t1\\n ...   \n",
       "\n",
       "                                  body_char_countvec  \n",
       "0    (0, 424)\\t1\\n  (0, 206)\\t5\\n  (0, 159)\\t3\\n ...  \n",
       "1    (0, 439)\\t8\\n  (0, 146)\\t5\\n  (0, 60)\\t1\\n  ...  \n",
       "2    (0, 159)\\t2\\n  (0, 431)\\t1\\n  (0, 276)\\t2\\n ...  \n",
       "3    (0, 416)\\t1\\n  (0, 290)\\t1\\n  (0, 252)\\t2\\n ...  \n",
       "4    (0, 424)\\t1\\n  (0, 431)\\t2\\n  (0, 416)\\t2\\n ...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_add3 = csr_matrix(X_word_grams)\n",
    "training['body_countvec'] = [row for row in to_add3]\n",
    "\n",
    "to_add4 = csr_matrix(X_char_grams)\n",
    "training['body_char_countvec'] = [row for row in to_add4]\n",
    "\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving ngrams results to pickle file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.to_pickle(\"../data/interim/train_data_w_ngrams.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
